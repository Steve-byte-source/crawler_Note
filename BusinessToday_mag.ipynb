{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note\n",
    "# 1._id = soup.select(\"meta[property='dable:item_id']\")[0].attrs[\"content\"]\n",
    "# 從    tag    <meta property='dable:item_id  content=\"201812220001\"> 中 , 取出  201812220001\n",
    "\n",
    "# 2.tag = soup.select(\"div[class='searchitem__content']\")[i].select(\"span\")[0].text\n",
    "# <span class=\"tag hot\">財經時事   <span class=\"tag hot person\">投資理財   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "# striptime function\n",
    "def time_t(time,t_type = \"%Y-%m-%d\"):\n",
    "    return datetime.datetime.strptime(time,t_type)\n",
    "\n",
    "# select last id\n",
    "def last_id():\n",
    "    path = \"C:/Users/user/Desktop/today_mag\"\n",
    "    last_id = os.listdir(path)\n",
    "    last_id.sort(reverse = True)\n",
    "    return last_id[0].split(\".\")[0]\n",
    "\n",
    "# Parameter = keyword\n",
    "def today_mag(key_word):\n",
    "    page = 1\n",
    "    url = f\"https://www.businesstoday.com.tw/group_search/article?keywords={keyword}\"\n",
    "    page_url = url + f\"&page={page}&order=new&field=title\"\n",
    "\n",
    "    res = requests.get(page_url)\n",
    "    soup = bs(res.text, 'html.parser')\n",
    "\n",
    "    pub_date = soup.select(\"p[class='searchitem__date']\")\n",
    "    pub_date_t = time_t(pub_date[0].text[3:])\n",
    "    \n",
    "    end_date_t = datetime.datetime.now()\n",
    "\n",
    "    while pub_date_t >= end_date_t:    \n",
    "        for i in range(len(pub_date)):\n",
    "            pub_date_t = time_t(pub_date[i].text[3:])\n",
    "            \n",
    "            # check publish date > end date\n",
    "            if pub_date_t >= end_date_t:\n",
    "                tag = soup.select(\"div[class='searchitem__content']\")[i].select(\"span\")[0].text\n",
    "                \n",
    "                #checking tag\n",
    "                if tag == '財經時事':\n",
    "                    # request Article\n",
    "                    article_url = soup.select(\"a[class='searchitem__title-link']\")[i].attrs['href']\n",
    "                    article_res = requests.get(article_url)\n",
    "                    article_soup = bs(article_res.text, 'html.parser')\n",
    "                    \n",
    "                    try:\n",
    "                        if last_id() != article_id:\n",
    "                            article_title = article_soup.select(\"title\")[0].text.replace(\"\\u3000\",\"\").split(\" -\")[0]\n",
    "                            article_id = article_soup.select(\"meta[property='dable:item_id']\")[0].attrs[\"content\"]\n",
    "\n",
    "                            article_publisher = \"今周刊\"\n",
    "                            article_pub_date = article_soup.select(\"meta[property='article:published_time']\")[0].attrs[\"content\"][0:10]\n",
    "                            article_writter = article_soup.select(\"meta[property='dable:author']\")[0].attrs[\"content\"]\n",
    "                            article_content = article_soup.select(\"div[itemprop=articleBody]\")[0].text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\xa0\",\"\").replace(\"\\u3000\",\"\")\n",
    "                            article_id = article_soup.select(\"meta[property='dable:item_id']\")[0].attrs[\"content\"]\n",
    "    #                             print(f\"{article_title}\\n{article_id} {article_publisher}\\n{article_pub_date} {article_writter}\\n\\n{article_content}\\n\\n\\n\")\n",
    "\n",
    "                            # saving file\n",
    "                            with open(f\"C:/Users/user/Desktop/today_mag/{article_id}.txt\",\"w\",encoding = \"utf-8\") as text: \n",
    "                                text.write(f\"Datetime:{article_pub_date}\\nSource:{article_publisher}\\nAuthor:{article_writter}\\ntitle:{article_title}\\n========\\n{article_content}\")\n",
    "                        else:\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                        \n",
    "                        \n",
    "        page_url = url + f\"&page={page}&order=new&field=title\"    \n",
    "        res = requests.get(page_url)\n",
    "        soup = bs(res.text, 'html.parser')\n",
    "        pub_date = soup.select(\"p[class='searchitem__date']\")\n",
    "        pub_date_t = time_t(pub_date[0].text[3:])\n",
    "        page += 1\n",
    "\n",
    "        \n",
    "        \n",
    "key_word = \"台積電\"\n",
    "\n",
    "today_mag(key_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download news in time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "\n",
    "# strptime function\n",
    "def time_t(time,t_type = \"%Y-%m-%d\"):\n",
    "    import datetime\n",
    "    return datetime.datetime.strptime(time,t_type)\n",
    "\n",
    "# last news id\n",
    "def last_id(path):\n",
    "    last_id = os.listdir(\"C:/Users/user/Desktop/t\")[-1].split(\".\")[0]\n",
    "    return last_id\n",
    "\n",
    "# Parameter 1 & 2  = keyword , tag\n",
    "# time range ,  Parameter 3 & 4  = start_date , end_date\n",
    "def today_mag(keyword,tag,start_date,end_date):\n",
    "    page = 1\n",
    "    url = f\"https://www.businesstoday.com.tw/group_search/article?keywords={keyword}\"\n",
    "    page_url = url + f\"&page={page}&order=new&field=title\"\n",
    "\n",
    "    res = requests.get(page_url)\n",
    "    soup = bs(res.text, 'html.parser')\n",
    "\n",
    "    pub_date = soup.select(\"p[class='searchitem__date']\")\n",
    "    pub_date_t = time_t(pub_date[0].text[3:])\n",
    "    \n",
    "    end_date_t = time_t(end_date)\n",
    "    start_date_t = time_t(start_date)\n",
    "\n",
    "    while pub_date_t >= end_date_t or pub_date_t >= start_date_t:    \n",
    "        for i in range(len(pub_date)):\n",
    "            pub_date_t = time_t(pub_date[i].text[3:])\n",
    "            #check   \" start_date =< pub_date =< end_date \"\n",
    "            if pub_date_t <= end_date_t:\n",
    "                if pub_date_t >= start_date_t: \n",
    "                    tag = soup.select(\"div[class='searchitem__content']\")[i].select(\"span\")[0].text\n",
    "                    #checking tag\n",
    "\n",
    "                    if tag == '財經時事':\n",
    "                        #request Article\n",
    "                        article_url = soup.select(\"a[class='searchitem__title-link']\")[i].attrs['href']\n",
    "                        article_res = requests.get(article_url)\n",
    "                        article_soup = bs(article_res.text, 'html.parser')\n",
    "\n",
    "\n",
    "                        try:\n",
    "                            article_title = article_soup.select(\"title\")[0].text.replace(\"\\u3000\",\"\").split(\" -\")[0]\n",
    "                            article_id = article_soup.select(\"meta[property='dable:item_id']\")[0].attrs[\"content\"]\n",
    "                            article_publisher = \"今周刊\"\n",
    "                            article_pub_date = article_soup.select(\"meta[property='article:published_time']\")[0].attrs[\"content\"][0:10]\n",
    "                            article_writter = article_soup.select(\"meta[property='dable:author']\")[0].attrs[\"content\"]\n",
    "                            article_content = article_soup.select(\"div[itemprop=articleBody]\")[0].text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\xa0\",\"\").replace(\"\\u3000\",\"\")\n",
    "                            article_id = article_soup.select(\"meta[property='dable:item_id']\")[0].attrs[\"content\"]\n",
    "                            print(f\"{article_title}\\n{article_id} {article_publisher}\\n{article_pub_date} {article_writter}\\n\\n{article_content}\\n\\n\\n\")\n",
    "                            with open(f\"C:/Users/user/Desktop/today_mag/{article_id}.txt\",\"w\",encoding = \"utf-8\") as text: \n",
    "                                text.write(f\"Datetime:{article_pub_date}\\nSource:{article_publisher}\\nAuthor:{article_writter}\\ntitle:{article_title}\\n========\\n{article_content}\")\n",
    "                        except IndexError:\n",
    "                            continue\n",
    "                            \n",
    "                            \n",
    "        page_url = url + f\"&page={page}&order=new&field=title\"    \n",
    "        res = requests.get(page_url)\n",
    "        soup = bs(res.text, 'html.parser')\n",
    "        pub_date = soup.select(\"p[class='searchitem__date']\")\n",
    "        pub_date_t = time_t(pub_date[0].text[3:])\n",
    "        page += 1\n",
    "\n",
    "tag = '財經時事'\n",
    "keyword = \"台積電\"\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2020-08-07'\n",
    "\n",
    "today_mag(keyword,tag,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
